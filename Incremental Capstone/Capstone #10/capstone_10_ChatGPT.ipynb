{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1322081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e3c41",
   "metadata": {},
   "source": [
    "Task: Build a Transfer Learning model to detect face masks on humans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415fccd3",
   "metadata": {},
   "source": [
    "Task A:\n",
    "•Load the Image Training and Test Datasets from the train and test folders respectively. The \n",
    "size of each image is 128 x 128 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fb571",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = '/home/cullen-fedora/Documents/With_mask_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = '/home/cullen-fedora/Documents/without_mask_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e382eec",
   "metadata": {},
   "source": [
    "•Load training dataset using Keras ImageDataGenerator with validation_split=0.2  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f80e0",
   "metadata": {},
   "source": [
    "•Load test dataset using Keras ImageDataGenerator \n",
    "Build a Transfer Learning network using Keras with the following layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050867fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to your dataset\n",
    "train_directory = train_file_path\n",
    "test_directory = test_file_path\n",
    "\n",
    "# Image dimensions\n",
    "image_size = (128, 128)  # Size of each image\n",
    "batch_size = 32           # Adjust batch size as needed\n",
    "\n",
    "# Set up ImageDataGenerator for training data with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,       # Normalize pixel values to [0, 1]\n",
    "    validation_split=0.2     # Specify validation split\n",
    ")\n",
    "\n",
    "# Load training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size=image_size,   # Resize images\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Use 'categorical' for multi-class; 'binary' for binary classification\n",
    "    subset='training',         # Set as training data\n",
    "    seed=42                   # Ensure reproducibility\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size=image_size,   # Resize images\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Use 'categorical' for multi-class\n",
    "    subset='validation',       # Set as validation data\n",
    "    seed=42                   # Ensure reproducibility\n",
    ")\n",
    "\n",
    "# Set up ImageDataGenerator for test data (no validation split needed)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)  # Only rescale for test data\n",
    "\n",
    "# Load test data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_directory,\n",
    "    target_size=image_size,    # Resize images\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',   # Multi-class expected\n",
    "    shuffle=False               # Don't shuffle test data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeba871",
   "metadata": {},
   "source": [
    "•Load EfficientNetB0 as first layers using Keras API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281a62a4",
   "metadata": {},
   "source": [
    "•GLobalAveragePooling2D layer\n",
    "\n",
    "•Dropout (0.2)\n",
    "\n",
    "•Dense layer with 3 neurons and activation SoftMax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EfficientNetB0 without top layers (include_top=False)\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Build the model using the Sequential API\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),  # Global average pooling to flatten the output\n",
    "    Dropout(0.2),              # Dropout for regularization\n",
    "    Dense(1, activation='softmax')  # Output layer for 3 classes (with_mask, without_mask, and possibly a third category)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feaac32",
   "metadata": {},
   "source": [
    "Compile the model with Adam optimizer, categorical_crossentropy loss and with metrics \n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba101a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd403b",
   "metadata": {},
   "source": [
    "•Train the model for 25 epochs with callbacks Reduce Learning Rate on Plateau and early stopping while monitoring validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bbabb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks: Reduce Learning Rate on Plateau and Early Stopping\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=25,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f'\\nTest accuracy: {test_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65484e8",
   "metadata": {},
   "source": [
    "•Plot training and validation accuracy and loss against epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34474a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation accuracy and loss\n",
    "def plot_training_history(history):\n",
    "    # Training and validation accuracy\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7147d5b",
   "metadata": {},
   "source": [
    "Task B\n",
    "•Load the Image Training and Test Datasets from the train and test folder respectively. \n",
    "The size of each image is 128 x 128 x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a8bc4f",
   "metadata": {},
   "source": [
    "•Load training dataset using Keras ImageDataGenerator with validation_split=0.2  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14302870",
   "metadata": {},
   "source": [
    "•Load test dataset using Keras ImageDataGenerator \n",
    "Build a Transfer Learning network using Keras with the following layers:\n",
    "\n",
    "•Load ResNet50 as first layers using Keras API\n",
    "\n",
    "•GLobalAveragePooling2D layer\n",
    "\n",
    "•Dropout(0.5)\n",
    "\n",
    "•Dense layer with 3 neurons and activation SoftMax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc369e7a",
   "metadata": {},
   "source": [
    "•Compile the model with Adam optimizer,categorical_crossentropy loss and with metrics \n",
    "accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480469bf",
   "metadata": {},
   "source": [
    "•Train the model for 25 epochs with callbacks Reduce Learning Rate on Plateau and early stopping while monitoring validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc299e3",
   "metadata": {},
   "source": [
    "•Plot training and validation accuracy and loss against epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc10dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_datagen = ImageDataGenerator(validation_split=0.2, rescale=1.0/255.0)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'train',                # Directory with training data\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'train',                # Same directory as training data\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'test',                 # Directory with testing data\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=25,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862b5298",
   "metadata": {},
   "source": [
    "Task C\n",
    "•Compare EfficientNetB0 and ResNet50 model performance and find the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa593551",
   "metadata": {},
   "source": [
    "•Using the best model, predict the test dataset and plot 10 images from the test set along with its True Label and Predicted Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbdb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load test dataset\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'test',                 # Directory with testing data\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False          # Important to retain the order of the images for predictions\n",
    ")\n",
    "\n",
    "# Evaluate EfficientNetB0 model\n",
    "efficientnetb0_eval = model.evaluate(test_generator)\n",
    "print(\"EfficientNetB0 Performance:\")\n",
    "print(f\"Loss: {efficientnetb0_eval[0]}; Accuracy: {efficientnetb0_eval[1]}\")\n",
    "\n",
    "# Assume `model_resnet50` is your ResNet50 model built previously\n",
    "resnet50_eval = model_resnet50.evaluate(test_generator)\n",
    "print(\"ResNet50 Performance:\")\n",
    "print(f\"Loss: {resnet50_eval[0]}; Accuracy: {resnet50_eval[1]}\")\n",
    "\n",
    "# Determine the better model based on accuracy\n",
    "best_model = model if efficientnetb0_eval[1] > resnet50_eval[1] else model_resnet50\n",
    "best_model_name = \"EfficientNetB0\" if efficientnetb0_eval[1] > resnet50_eval[1] else \"ResNet50\"\n",
    "print(f\"The best model is: {best_model_name}\")\n",
    "\n",
    "# Predict on the test dataset using the best model\n",
    "test_generator.reset()  # Reset the generator for prediction\n",
    "predictions = best_model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Plot 10 images with true and predicted labels\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(test_generator[i][0][0])\n",
    "    plt.axis('off')\n",
    "    plt.title(f'True: {class_labels[true_classes[i]]}\\nPred: {class_labels[predicted_classes[i]]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
