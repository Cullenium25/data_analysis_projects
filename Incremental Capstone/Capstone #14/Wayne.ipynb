{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2f807f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cullen-fedora/Documents/Python-Projects/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Updated HuggingFace Setup with Error Handling and Model Selection\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a980f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":woman-biking: BikeEase HuggingFace Model Setup\n",
      "==================================================\n",
      ":mag: Checking system requirements...\n",
      "PyTorch version: 2.7.0+cu126\n",
      ":warning: CUDA not available - using CPU\n",
      "System RAM: 92.1 GB\n",
      ":bulb: Recommended model: facebook/opt-1.3b (high quality)\n",
      "\n",
      ":dart: Setting up with recommended model: facebook/opt-1.3b\n",
      ":arrows_counterclockwise: Loading model: facebook/opt-1.3b\n",
      "This may take a few minutes for first-time download...\n",
      ":computer: Using device: cpu\n",
      ":memo: Loading tokenizer...\n",
      ":brain: Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=100) and `max_length`(=257) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":wrench: Setting up pipeline...\n",
      ":white_check_mark: Model loaded successfully!\n",
      "\n",
      ":test_tube: Testing model generation...\n",
      ":memo: Generated text:\n",
      "----------------------------------------\n",
      "Create a short advertisement for a mountain bike rental with 25% discount.\n",
      "    Advertisement:                                                                                                    \n",
      "----------------------------------------\n",
      ":white_check_mark: Test successful!\n",
      "\n",
      ":tada: Setup complete! You can now use:\n",
      "- llm: LangChain HuggingFace pipeline\n",
      "- tokenizer: HuggingFace tokenizer\n",
      "- model: Raw HuggingFace model\n"
     ]
    }
   ],
   "source": [
    "def setup_huggingface_model(model_name=\"microsoft/DialoGPT-medium\"):\n",
    "  \"\"\"\n",
    "  Setup HuggingFace model with proper error handling\n",
    "  Recommended models by size:\n",
    "  - \"distilgpt2\" (~320MB) - Fastest, good for testing\n",
    "  - \"microsoft/DialoGPT-medium\" (~350MB) - Good balance\n",
    "  - \"gpt2\" (~500MB) - Better quality\n",
    "  - \"microsoft/DialoGPT-large\" (~750MB) - High quality\n",
    "  - \"facebook/opt-1.3b\" (~2.6GB) - Best quality, requires more RAM\n",
    "  \"\"\"\n",
    "  print(f\":arrows_counterclockwise: Loading model: {model_name}\")\n",
    "  print(\"This may take a few minutes for first-time download...\")\n",
    "  try:\n",
    "    # Check if CUDA is available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\":computer: Using device: {device}\")\n",
    "    # Load tokenizer\n",
    "    print(\":memo: Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Add padding token if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "      tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Load model with appropriate settings\n",
    "    print(\":brain: Loading model...\")\n",
    "    if device == \"cuda\":\n",
    "      # GPU setup\n",
    "      model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16, # Use half precision to save memory\n",
    "        device_map=\"auto\", # Automatically handle device placement\n",
    "        low_cpu_mem_usage=True # Reduce CPU memory usage\n",
    "      )\n",
    "    else:\n",
    "      # CPU setup\n",
    "      model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32, # Full precision for CPU\n",
    "        low_cpu_mem_usage=True\n",
    "      )\n",
    "    # Create text generation pipeline\n",
    "    print(\":wrench: Setting up pipeline...\")\n",
    "    generator = pipeline(\n",
    "      'text-generation',\n",
    "      model=model,\n",
    "      tokenizer=tokenizer,\n",
    "      max_length=256, # Adjust based on your needs\n",
    "      max_new_tokens=100, # Maximum tokens to generate\n",
    "      temperature=0.7, # Creativity level (0.1-1.0)\n",
    "      do_sample=True, # Enable sampling\n",
    "      pad_token_id=tokenizer.eos_token_id,\n",
    "      device=0 if device == \"cuda\" else -1\n",
    "    )\n",
    "    # Wrap in LangChain\n",
    "    llm = HuggingFacePipeline(pipeline=generator)\n",
    "    print(\":white_check_mark: Model loaded successfully!\")\n",
    "    return llm, tokenizer, model\n",
    "  except Exception as e:\n",
    "    print(f\":x: Error loading model {model_name}: {e}\")\n",
    "    print(\":bulb: Try a smaller model like 'distilgpt2' or 'microsoft/DialoGPT-medium'\")\n",
    "    return None, None, None\n",
    "def test_model_generation(llm):\n",
    "  \"\"\"Test the model with a simple prompt\"\"\"\n",
    "  if llm is None:\n",
    "    print(\":x: No model available for testing\")\n",
    "    return\n",
    "  print(\"\\n:test_tube: Testing model generation...\")\n",
    "  # Create a simple prompt template\n",
    "  prompt_template = PromptTemplate(\n",
    "    input_variables=[\"bike_type\", \"discount\"],\n",
    "    template=\"\"\"Create a short advertisement for a {bike_type} bike rental with {discount}% discount.\n",
    "    Advertisement:\"\"\"\n",
    "  )\n",
    "  # Create LangChain chain\n",
    "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "  try:\n",
    "    # Generate response\n",
    "    result = chain.run(bike_type=\"mountain\", discount=25)\n",
    "    print(\":memo: Generated text:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result)\n",
    "    print(\"-\" * 40)\n",
    "    print(\":white_check_mark: Test successful!\")\n",
    "  except Exception as e:\n",
    "    print(f\":x: Generation error: {e}\")\n",
    "def check_system_requirements():\n",
    "  \"\"\"Check system capabilities\"\"\"\n",
    "  print(\":mag: Checking system requirements...\")\n",
    "  # Check PyTorch installation\n",
    "  print(f\"PyTorch version: {torch.__version__}\")\n",
    "  # Check CUDA availability\n",
    "  if torch.cuda.is_available():\n",
    "    print(f\":white_check_mark: CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "  else:\n",
    "    print(\":warning: CUDA not available - using CPU\")\n",
    "  # Check available RAM (approximate)\n",
    "  import psutil\n",
    "  ram_gb = psutil.virtual_memory().total / (1024 ** 3)\n",
    "  print(f\"System RAM: {ram_gb:.1f} GB\")\n",
    "  # Recommend model based on system\n",
    "  if ram_gb < 8:\n",
    "    recommended = \"distilgpt2\"\n",
    "    print(\":bulb: Recommended model: distilgpt2 (lightweight)\")\n",
    "  elif ram_gb < 16:\n",
    "    recommended = \"microsoft/DialoGPT-medium\"\n",
    "    print(\":bulb: Recommended model: microsoft/DialoGPT-medium\")\n",
    "  else:\n",
    "    recommended = \"facebook/opt-1.3b\"\n",
    "    print(\":bulb: Recommended model: facebook/opt-1.3b (high quality)\")\n",
    "  return recommended\n",
    "def main():\n",
    "  \"\"\"Main setup and testing function\"\"\"\n",
    "  print(\":woman-biking: BikeEase HuggingFace Model Setup\")\n",
    "  print(\"=\" * 50)\n",
    "  # Check system requirements\n",
    "  recommended_model = check_system_requirements()\n",
    "  print(f\"\\n:dart: Setting up with recommended model: {recommended_model}\")\n",
    "  # Setup model\n",
    "  llm, tokenizer, model = setup_huggingface_model(recommended_model)\n",
    "  if llm is not None:\n",
    "    # Test the model\n",
    "    test_model_generation(llm)\n",
    "    print(\"\\n:tada: Setup complete! You can now use:\")\n",
    "    print(\"- llm: LangChain HuggingFace pipeline\")\n",
    "    print(\"- tokenizer: HuggingFace tokenizer\")\n",
    "    print(\"- model: Raw HuggingFace model\")\n",
    "    return llm, tokenizer, model\n",
    "  else:\n",
    "    print(\"\\n:x: Setup failed. Please check the error messages above.\")\n",
    "    return None, None, None\n",
    "if __name__ == \"__main__\":\n",
    "  llm, tokenizer, model = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
