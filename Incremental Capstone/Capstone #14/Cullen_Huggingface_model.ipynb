{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155cc689",
   "metadata": {},
   "source": [
    "# Overview\n",
    "BikeEase has successfully implemented various AI-powered solutions for demand forecasting, customer review analysis, and image classification. As they continue to grow, they aim to automate certain tasks using Large Language Models (LLMs), particularly in marketing and advertising generation to attract more customers and increase engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7c7fe",
   "metadata": {},
   "source": [
    "To achieve this, BikeEase plans to develop a Generative AI-powered system that can automatically create engaging and persuasive advertisements based on bike specifications, discount offers, and promotional themes. This will enable them to generate high-quality marketing content without manual effort, saving time and ensuring brand consistency\n",
    "\n",
    "Project Statement\n",
    "\n",
    "Develop a Generative AI-powered advertisement generation system using LLMs and LangChain to create compelling promotional content for BikeEaseâ€™s rental services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d08e88",
   "metadata": {},
   "source": [
    "Steps to Perform\n",
    "\n",
    "Task 1: Understand generative AI & LLMs\n",
    "\n",
    "Explore how LLMs can be used for automated marketing\n",
    "Learn about LangChain and how it helps integrate LLMs into applications\n",
    "Task 2: Designing the Ad generation pipeline\n",
    "\n",
    "Accept user inputs for bike specifications, discount options, and marketing themes\n",
    "Use LLMs (Hugging Face models) to generate creative, engaging ads\n",
    "Structure the output to align with BikeEaseâ€™s branding and tone\n",
    "Task 3: Building the LLM-based Ad generator\n",
    "\n",
    "Use LangChain to manage the prompt engineering process\n",
    "Integrate a local Hugging Face model to generate text without API dependencies\n",
    "Experiment with different prompt techniques to enhance response quality\n",
    "Task 4: Evaluation and optimization\n",
    "\n",
    "Test the ad variations to ensure quality, persuasiveness, and relevance.\n",
    "Implement prompt tuning to fine-tune outputs for different use cases.\n",
    "Compare different LLM models to identify the most effective one for marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437f14bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cullen-fedora/Documents/Python-Projects/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb454e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee757cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# # Login (can also be CLI or environment variable)\n",
    "# login(token=\"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\")\n",
    "\n",
    "# model_name = \"google/gemma-7b\"  # Choose a model that runs on your system\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f3861fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a local model from Hugging Face (No API token required)\n",
    "model_name = \"gpt2\"  # Choose a model that runs on your system\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3d4d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.75,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f5bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_502/1933646148.py:2: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  local_llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Wrap pipeline in a LangChain LLM\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d103d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"bike_model\", \"features\", \"discount\", \"theme\"],\n",
    "    template=\"\"\"\n",
    "Create a compelling marketing advertisement for BikeEase based on the following:\n",
    "- Bike Model: {bike_model}\n",
    "- Features: {features}\n",
    "- Discount: {discount}\n",
    "- Marketing Theme: {theme}\n",
    "\n",
    "Write in a friendly, energetic, and persuasive tone:\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9fb683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_502/217512142.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(prompt=prompt_template, llm=local_llm)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create the LangChain chain\n",
    "llm_chain = LLMChain(prompt=prompt_template, llm=local_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b51815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Input example\n",
    "input_data = {\n",
    "    \"bike_model\": \"Schwinn Cycles E-Bike 2023\",\n",
    "    \"features\": \"lightweight aluminum frame, 40-mile battery range, smart lock system\",\n",
    "    \"discount\": \"20% off this weekend only\",\n",
    "    \"theme\": \"eco-friendly urban commuting\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c227b1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_502/135285800.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  generated_ad = llm_chain.run(input_data)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“£ Generated Ad:\n",
      " \n",
      "Create a compelling marketing advertisement for BikeEase based on the following:\n",
      "- Bike Model: Schwinn Cycles E-Bike 2023\n",
      "- Features: lightweight aluminum frame, 40-mile battery range, smart lock system\n",
      "- Discount: 20% off this weekend only\n",
      "- Marketing Theme: eco-friendly urban commuting\n",
      "\n",
      "Write in a friendly, energetic, and persuasive tone:\n",
      "\n",
      "\"I love the fact that I was able to build a bike with my own hands and without the help of a third party. I can't wait to take my first ride and it will be a big part of my life.\"\n",
      "\n",
      "- Michael Lait, CEO, BikeEase\n",
      "\n",
      "BikeEase's bike shop has been built for the environment. We have a sustainable and eco-friendly brand that can help you bring your bike to the next level. We encourage you to take part in BikeEase's Bike Model program by registering your bike at BikeEase.com/bike-model. Please note: Please note that if you register your bike before 7PM on Saturday, July 11th you will be automatically notified via email.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Generate the ad\n",
    "generated_ad = llm_chain.run(input_data)\n",
    "print(\"ðŸ“£ Generated Ad:\\n\", generated_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022b40a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/tmp/ipykernel_676/4154110113.py:15: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  output_2 = hf_pipeline_2(\"Translate 'Hello' to French:\") # This task won't work well with a text-generation model\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate 'Hello' to French:\n",
      "\n",
      "'Hello' = \"Hello\"\n",
      "\n",
      "This is also used to translate the value to English:\n",
      "\n",
      "'Hello' = \"Hello\"\n",
      "\n",
      "If you want to use the French translation,\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# # Option 1: Using model_id and task\n",
    "# hf_pipeline_1 = HuggingFacePipeline(model_id=\"gpt2\", task=\"text-generation\", pipeline_kwargs={'max_length': 50, 'temperature': 0.7})\n",
    "\n",
    "# Using the Langchain HuggingFacePipeline\n",
    "# output_1 = hf_pipeline_1(\"Write a short story about a cat who can talk:\")\n",
    "# print(output_1)\n",
    "\n",
    "# Option 2: Passing an existing transformers pipeline\n",
    "transformers_pipeline = pipeline(\"text-generation\", model=\"gpt2\", max_length=50, temperature=0.7, device=0) # Assuming GPU is available\n",
    "hf_pipeline_2 = HuggingFacePipeline(pipeline=transformers_pipeline) \n",
    "\n",
    "output_2 = hf_pipeline_2(\"Translate 'Hello' to French:\") # This task won't work well with a text-generation model\n",
    "print(output_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
