{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0034d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data (run this once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e399c",
   "metadata": {},
   "source": [
    "## Task 1: Data Collection & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc03a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Loads data, cleans text, and prepares it for modeling.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Assuming your dataset has a column named 'review_text' and 'sentiment' (adjust as needed)\n",
    "        df = df[['review_text', 'sentiment']].dropna()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "    # Text Cleaning\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        text = ' '.join([lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words])\n",
    "        return text\n",
    "\n",
    "    df['cleaned_text'] = df['review_text'].apply(clean_text)\n",
    "\n",
    "    # Assuming your 'sentiment' column needs encoding (e.g., categorical to numerical)\n",
    "    # Adjust this based on your actual 'sentiment' column values\n",
    "    sentiment_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "    df['sentiment_encoded'] = df['sentiment'].map(sentiment_mapping).fillna(0) # Fill NaN with neutral\n",
    "\n",
    "    return df[['cleaned_text', 'sentiment_encoded']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23415f9e",
   "metadata": {},
   "source": [
    "## Task 2: \n",
    "### Sentiment Analysis - Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e049f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_traditional(df, model_type='logistic_regression'):\n",
    "    \"\"\"Trains and evaluates a traditional sentiment analysis model.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Vectorization using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "    if model_type == 'logistic_regression':\n",
    "        model = LogisticRegression(random_state=42)\n",
    "    elif model_type == 'naive_bayes':\n",
    "        model = MultinomialNB()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model_type: {model_type}. Choose 'logistic_regression' or 'naive_bayes'.\")\n",
    "\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"--- {model_type.capitalize()} Model Evaluation ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-score (weighted): {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "                yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'{model_type.capitalize()} Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8f6311",
   "metadata": {},
   "source": [
    "### Sentiment Analysis - Deep Learning Models (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64533227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_lstm(df):\n",
    "    \"\"\"Trains and evaluates an LSTM sentiment analysis model.\"\"\"\n",
    "    X_train_text, X_test_text, y_train, y_test = train_test_split(df['cleaned_text'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(num_words=10000, oov_token=\"<unk>\") # Adjust num_words as needed\n",
    "    tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "    # Padding\n",
    "    maxlen = 100 # Adjust maxlen as needed\n",
    "    X_train_padded = pad_sequences(X_train_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "    X_test_padded = pad_sequences(X_test_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "    # Model Building\n",
    "    embedding_dim = 16 # Adjust embedding dimension as needed\n",
    "    lstm_units = 32 # Adjust LSTM units as needed\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=10000, output_dim=embedding_dim, input_length=maxlen),\n",
    "        LSTM(units=lstm_units),\n",
    "        Dense(units=3, activation='softmax') # 3 output units for negative, neutral, positive\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(\"--- LSTM Model Summary ---\")\n",
    "    model.summary()\n",
    "\n",
    "    # Training\n",
    "    epochs = 5 # Adjust number of epochs as needed\n",
    "    batch_size = 32 # Adjust batch size as needed\n",
    "    history = model.fit(X_train_padded, y_train + 1, # Adjust labels to start from 0 for sparse_categorical_crossentropy\n",
    "                        epochs=epochs, batch_size=batch_size,\n",
    "                        validation_data=(X_test_padded, y_test + 1))\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred_probs = model.predict(X_test_padded)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1) - 1 # Convert back to original labels\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n--- LSTM Model Evaluation ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-score (weighted): {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "                yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('LSTM Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('LSTM Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('LSTM Loss')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aee3cb",
   "metadata": {},
   "source": [
    "### Task 2: Sentiment Analysis - Deep Learning Models (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399e250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_bert(df, model_name='bert-base-uncased'):\n",
    "    \"\"\"Trains and evaluates a BERT sentiment analysis model.\"\"\"\n",
    "    X_train_text, X_test_text, y_train, y_test = train_test_split(df['cleaned_text'], df['sentiment_encoded'], test_size=0.2, random_state=42)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=3) # 3 labels: negative, neutral, positive\n",
    "\n",
    "    train_encodings = tokenizer(list(X_train_text), truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(list(X_test_text), truncation=True, padding=True)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(train_encodings),\n",
    "        np.array(y_train + 1) # Adjust labels to start from 0\n",
    "    )).shuffle(len(X_train_text)).batch(8) # Adjust batch size\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(test_encodings),\n",
    "        np.array(y_test + 1)\n",
    "    )).batch(8)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    def train_step(batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(batch[0])\n",
    "            loss = loss_fn(batch[1], outputs.logits)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        metric.update_state(batch[1], outputs.logits)\n",
    "        return loss\n",
    "\n",
    "    def evaluate_step(batch):\n",
    "        outputs = model(batch[0], training=False)\n",
    "        loss = loss_fn(batch[1], outputs.logits)\n",
    "        metric.update_state(batch[1], outputs.logits)\n",
    "        return loss\n",
    "\n",
    "    epochs = 2 # Adjust number of epochs\n",
    "    print(\"--- BERT Model Training ---\")\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        for step, batch in enumerate(train_dataset):\n",
    "            loss = train_step(batch)\n",
    "            if step % 100 == 0:\n",
    "                print(f\"  Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {metric.result().numpy():.4f}\")\n",
    "        print(f\"  Epoch {epoch + 1} Training Accuracy: {metric.result().numpy():.4f}\")\n",
    "        metric.reset_states()\n",
    "\n",
    "        for batch in test_dataset:\n",
    "            loss = evaluate_step(batch)\n",
    "        print(f\"  Epoch {epoch + 1} Evaluation Accuracy: {metric.result().numpy():.4f}\")\n",
    "        metric.reset_states()\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred_logits = model.predict(test_dataset).logits\n",
    "    y_pred = np.argmax(y_pred_logits, axis=1) - 1\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n--- BERT Model Evaluation ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-score (weighted): {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "                yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('BERT Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_dataset.csv' with the actual path to your dataset file\n",
    "    file_path = 'your_dataset.csv'\n",
    "    processed_df = load_and_preprocess_data(file_path)\n",
    "\n",
    "    if processed_df is not None:\n",
    "        print(\"--- Processed Data Sample ---\")\n",
    "        print(processed_df.head())\n",
    "\n",
    "        # Train and evaluate traditional models\n",
    "        train_and_evaluate_traditional(processed_df, model_type='logistic_regression')\n",
    "        train_and_evaluate_traditional(processed_df, model_type='naive_bayes')\n",
    "\n",
    "        # Train and evaluate LSTM model\n",
    "        train_and_evaluate_lstm(processed_df)\n",
    "\n",
    "        # Train and evaluate BERT model\n",
    "        train_and_evaluate_bert(processed_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
